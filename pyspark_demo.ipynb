{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code that accompanies my blog on PySpark. The blog post is located [here](https://zach-a-greenberg.medium.com/apache-spark-with-pyspark-60b01bcc089)\n",
    "\n",
    "For this demo, I am going to use a dataset I created via webscrapping (see this [blog](https://zach-a-greenberg.medium.com/web-scraping-with-python-using-beautifulsoup-d8472555fcb5)). While this dataset is not large and I am not actually connected to the cloud, the same coding principles apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Spark, we need to establish a connection. The connection is made in the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has it's own way of reading in the data. The parameter header=True makes the assumption that the first row contains the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the DataFrame with Spark\n",
    "broadway = spark.read.csv('broadway.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "|_c0|               Shows|         Description|             Theater|Current Ticket Cost|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "|  0|             Wicked |Meet the witches ...|    Gershwin Theatre|              89.00|\n",
      "|  1|Moulin Rouge! The...|A theatrical cele...|Al Hirschfeld The...|              69.00|\n",
      "|  2|      The Lion King |Pride Rock comes ...|    Minskoff Theatre|              75.00|\n",
      "|  3|To Kill a Mocking...|Harper Lee’s clas...|     Shubert Theatre|              29.00|\n",
      "|  4|           Hamilton |A fresh look at t...|Richard Rodgers T...|             149.00|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadway.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we are dropping the 'index column' from the original dataset. Just like in Pandas, we can do this inplace by overwriting the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the _c0 or index column\n",
    "broadway = broadway.drop('_c0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, the printSchema method is the way to check the datatypes. We can also check to see if null values are allowed in the data from the nullable argument. m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Shows: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Theater: string (nullable = true)\n",
      " |-- Current Ticket Cost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the datatypes\n",
    "broadway.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When changing a datatype in Spark, we are essentially overwriting a column and using the cast method to make the datatype conversion. Double is Spark's version of a float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the cost datatype to a 'double' which is the equivalent of a decimal\n",
    "broadway = broadway.withColumn('Current Ticket Cost', broadway['Current Ticket Cost'].cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When formatting text columns, similar principles apply. In a few steps we can modify the text using the functions provided in the pyspark.sql module. A list of them can be found [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+-------------------+\n",
      "|               Shows|         Description|       Theater|Current Ticket Cost|\n",
      "+--------------------+--------------------+--------------+-------------------+\n",
      "|             Wicked |Meet the witches ...|     Gershwin |               89.0|\n",
      "|Moulin Rouge! The...|A theatrical cele...|Al Hirschfeld |               69.0|\n",
      "|      The Lion King |Pride Rock comes ...|     Minskoff |               75.0|\n",
      "+--------------------+--------------------+--------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping the word 'Theatre' from all of the entries in the Theater column\n",
    "\n",
    "#we create a column to perform the split of text\n",
    "broadway = broadway.withColumn('Split', F.split(broadway.Theater, 'Theatre'))\n",
    "\n",
    "#we overwrite the original column with the desired text part\n",
    "broadway = broadway.withColumn('Theater', broadway.Split.getItem(0))\n",
    "\n",
    "#we drop the column used to make the split\n",
    "broadway = broadway.drop('Split')\n",
    "\n",
    "broadway.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS! A parquet is a special Spark DataFrame that allows use to quickly analyze the data. It is wise to create a parquet version of a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order of make a parquet, the columns CANNOT have any spaces in them so I will rename them\n",
    "broadway = broadway.withColumnRenamed('Current Ticket Cost', 'Cost')\n",
    "\n",
    "#rewriting the dataframe as a parquet, a Spark specific file enabling for faster analysis\n",
    "broadway.write.parquet('broadway.parquet', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows = spark.read.parquet('broadway.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Shows: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Theater: string (nullable = true)\n",
      " |-- Cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a parquet, we can perform some simple analysis. Just like Pandas, we can use the describe method to see some summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------+-----------------+\n",
      "|summary|               Shows|         Description|       Theater|             Cost|\n",
      "+-------+--------------------+--------------------+--------------+-----------------+\n",
      "|  count|                  24|                  24|            24|               24|\n",
      "|   mean|                null|                null|          null|64.20833333333333|\n",
      "| stddev|                null|                null|          null|26.12466628314699|\n",
      "|    min|Ain't Too Proud –...|A deeply personal...|Al Hirschfeld |             29.0|\n",
      "|    max|             Wicked |When the world st...|Winter Garden |            149.0|\n",
      "+-------+--------------------+--------------------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#getting the summary stats of all the columns, the only one we really care about is Cost\n",
    "shows.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the parquet into a SQL table to write SQL queries for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporarily making the parquet into a SQL table for querying\n",
    "shows.createOrReplaceTempView('top25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Shows='Hamilton ', Cost=149.0),\n",
       " Row(Shows='The Music Man ', Cost=99.0),\n",
       " Row(Shows='Wicked ', Cost=89.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#writing a SQL query for the top 3 most expensive shows\n",
    "spark.sql('SELECT Shows, Cost FROM top25 WHERE Cost > 65.00 ORDER BY Cost DESC LIMIT 3').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the filter method on the parquet to filter the data to find specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               Shows|Cost|\n",
      "+--------------------+----+\n",
      "|To Kill a Mocking...|29.0|\n",
      "|            Aladdin |57.5|\n",
      "|          Hadestown |49.0|\n",
      "|            Chicago |49.5|\n",
      "|The Phantom of th...|29.0|\n",
      "|     Come From Away |49.0|\n",
      "|David Byrne's Ame...|59.0|\n",
      "|Ain't Too Proud –...|39.0|\n",
      "|     Mrs. Doubtfire |49.0|\n",
      "| Jagged Little Pill |49.0|\n",
      "|            Company |59.0|\n",
      "|Girl From the Nor...|39.0|\n",
      "|     MJ The Musical |59.0|\n",
      "|              Diana |49.0|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using the filtering method to find shows that cost less than $60\n",
    "shows['Shows', 'Cost'].filter(shows.Cost < 60).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
